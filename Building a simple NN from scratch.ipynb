{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Building a simple NN from scratch.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMD4dVE4fpuJ5lLe8llYe+t"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"nWvW_WOStLA-"},"source":["introduire pr√©sentation et mettre une image de l'objectif\n","\n"]},{"cell_type":"markdown","metadata":{"id":"59-Bk_Aezbgq"},"source":["# MNIST data set"]},{"cell_type":"markdown","metadata":{"id":"Zn6nTxVNzwVz"},"source":["We use fast.ai to download the MNIST data set"]},{"cell_type":"code","metadata":{"id":"6VfjYd1tzDJ0"},"source":["!pip install -Uqq fastbook\n","import fastbook\n","fastbook.setup_book()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WwDmEc0DzDSs"},"source":["from fastai.vision.all import *\n","from fastbook import *"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mXRr2hfwyjFK","colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"status":"ok","timestamp":1602962339891,"user_tz":-120,"elapsed":6396,"user":{"displayName":"Raphael Catanese","photoUrl":"","userId":"01157269626718155422"}},"outputId":"739b04ff-079c-41f7-c349-961b834b5862"},"source":["path = untar_data(URLs.MNIST_SAMPLE)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":[""],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"gY_MV6yW0emN"},"source":["We downloaded a sample selected by Fast.ai of the MNIST data containing only threes and sevens. let's look at what is inside"]},{"cell_type":"code","metadata":{"id":"ZIK-Xza30DlV","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1602962339893,"user_tz":-120,"elapsed":6391,"user":{"displayName":"Raphael Catanese","photoUrl":"","userId":"01157269626718155422"}},"outputId":"06eee34b-0cd3-4229-bd61-9f83d5e998a8"},"source":["path.ls()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(#3) [Path('/root/.fastai/data/mnist_sample/valid'),Path('/root/.fastai/data/mnist_sample/train'),Path('/root/.fastai/data/mnist_sample/labels.csv')]"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"htBwwNwe04Ry"},"source":["Let's clean it a bit :"]},{"cell_type":"code","metadata":{"id":"-xxeWpLk0SCF"},"source":["Path.BASE_PATH = path"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CxW8Msom08Ys","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1602962339894,"user_tz":-120,"elapsed":6375,"user":{"displayName":"Raphael Catanese","photoUrl":"","userId":"01157269626718155422"}},"outputId":"ac4418f6-f0bd-4fb9-ff2c-111e747e9c91"},"source":["path.ls()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(#3) [Path('valid'),Path('train'),Path('labels.csv')]"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"HjwJWzG61CJ0","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1602962339894,"user_tz":-120,"elapsed":6367,"user":{"displayName":"Raphael Catanese","photoUrl":"","userId":"01157269626718155422"}},"outputId":"feab4e8c-5881-47d8-cfce-a63cb66cb6f3"},"source":["(path/'train').ls()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(#2) [Path('train/7'),Path('train/3')]"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"xQWOVpGSFIe1"},"source":["# Data Preprocessing : preparing test and validation dataset"]},{"cell_type":"markdown","metadata":{"id":"0EqC3Op4EuXM"},"source":["We put the training data into variables : "]},{"cell_type":"code","metadata":{"id":"hmkueLJEEhGV","colab":{"base_uri":"https://localhost:8080/","height":55},"executionInfo":{"status":"ok","timestamp":1602962339895,"user_tz":-120,"elapsed":6359,"user":{"displayName":"Raphael Catanese","photoUrl":"","userId":"01157269626718155422"}},"outputId":"dd5ba5ec-1a88-4755-8c73-d63fd8e886fb"},"source":["threes = (path/'train/3').ls()\n","sevens = (path/'train/7').ls()\n","sevens"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(#6265) [Path('train/7/33681.png'),Path('train/7/40214.png'),Path('train/7/3788.png'),Path('train/7/42544.png'),Path('train/7/47325.png'),Path('train/7/6147.png'),Path('train/7/617.png'),Path('train/7/51138.png'),Path('train/7/56567.png'),Path('train/7/37835.png')...]"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"7MMEbzeuGbUC"},"source":["And put all the images into tensors so that we can use is in our model. To do so we use list comprehension : "]},{"cell_type":"code","metadata":{"id":"RGdlqPe6GaWb","colab":{"base_uri":"https://localhost:8080/","height":552},"executionInfo":{"status":"ok","timestamp":1602962342757,"user_tz":-120,"elapsed":9212,"user":{"displayName":"Raphael Catanese","photoUrl":"","userId":"01157269626718155422"}},"outputId":"d16afc62-33b7-4aba-82ce-f1a99449f6fb"},"source":["train_three_tensors = [tensor(Image.open(o)) for o in threes]\n","train_seven_tensors = [tensor(Image.open(o)) for o in sevens]\n","print(len(train_seven_tensors))\n","train_seven_tensors[0]"],"execution_count":null,"outputs":[{"output_type":"stream","text":["6265\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","        [  0,   0,   0,   0,   0,   0,  38, 145, 227, 199, 231, 254, 254, 254, 254, 188,  40,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","        [  0,   0,   0,   0,   0,  21, 240, 252, 243, 243, 243, 243, 243, 243, 244, 253, 173,   3,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","        [  0,   0,   0,   0,  17, 193, 253,  95,   0,   0,   0,   0,   0,   0,   6, 199, 253, 100,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","        [  0,   0,   0,   0,  60, 226,  66,   4,   0,   0,   0,   0,   0,   0,   0, 162, 253, 173,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  50, 245, 173,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 224, 183,   3,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 224, 253,  20,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 224, 253,  20,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 185, 253,  20,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 224, 238,  16,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 224, 173,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 224, 173,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   5, 226, 173,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  70, 253, 173,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 120, 254, 173,  10,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 169, 253, 128, 137,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 169, 254, 246, 127,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 169, 254, 253,  45,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 169, 254, 159,   1,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   6, 203, 244,  45,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]], dtype=torch.uint8)"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"foluIa4NOEEU"},"source":["We regroup all the tensors of our lists into two tensors of rank 3"]},{"cell_type":"code","metadata":{"id":"AFkdqj_vOEM8","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1602962342758,"user_tz":-120,"elapsed":9204,"user":{"displayName":"Raphael Catanese","photoUrl":"","userId":"01157269626718155422"}},"outputId":"5d0abc57-ef51-41f7-91ed-1635400ca087"},"source":["train_threes = torch.stack(train_three_tensors).float()/255 #the data type was uint8 which is not comptatible with matrice multiplication (we need it later)\n","train_sevens = torch.stack(train_seven_tensors).float()/255\n","train_sevens.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([6265, 28, 28])"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"8xj6GJzkJd02"},"source":["Now we repeat the same steps for our validation dataset."]},{"cell_type":"code","metadata":{"id":"ce9252JDH1Gc","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1602967531879,"user_tz":-120,"elapsed":1517,"user":{"displayName":"Raphael Catanese","photoUrl":"","userId":"01157269626718155422"}},"outputId":"2ed59a47-4d48-4a07-a5e8-a1f405687b4f"},"source":["valid_threes = torch.stack([tensor(Image.open(o)) for o in (path/\"valid/3\").ls()]).float()/255\n","valid_sevens = torch.stack([tensor(Image.open(o)) for o in (path/\"valid/7\").ls()]).float()/255\n","\n","len(valid_sevens)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1028"]},"metadata":{"tags":[]},"execution_count":293}]},{"cell_type":"markdown","metadata":{"id":"98rWVUbfH1MK"},"source":["Now we will concatenate the sevens and threes tensors into a training and a validation tensors. We will call them train_x and valid_x. The \"x\" means the rank 2 tensors are the dependent variable of our model (the variable on which our model will train to predict the independant variable or label, which is commonly written as \"y\") "]},{"cell_type":"code","metadata":{"id":"H8V2ZjgeH1Rs"},"source":["train_x = torch.cat([train_threes,train_sevens])\n","valid_x = torch.cat([valid_threes,valid_sevens])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I6OCYrdSPmN-"},"source":["Now we create the label of our data. We label all the threes by 1 and all the sevens by 0."]},{"cell_type":"code","metadata":{"id":"vrmV1h9gPmUV"},"source":["train_y = tensor([1]*len(train_threes)+[0]*len(train_sevens))\n","valid_y = tensor ([1]*len(valid_threes)+ [0]*len(valid_sevens))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E_oe8JpHRcCU","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1602962343530,"user_tz":-120,"elapsed":9945,"user":{"displayName":"Raphael Catanese","photoUrl":"","userId":"01157269626718155422"}},"outputId":"b9ea6af3-da45-49bd-8971-d9e85778cd9b"},"source":["train_x.shape, train_y.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([12396, 28, 28]), torch.Size([12396]))"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"mq1ub_LPRbAn"},"source":["We now have a problem. we want to make a list of tuple (x,y) but the shape of train_x and train_y are too different. \n","\n","To solve this problem we will transform the 28 by 28 matrices into long vectors of 784 elements and transform our label vector into a matrice of 12396 rows and 1 column."]},{"cell_type":"code","metadata":{"id":"m7jMpgDBRbQD","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1602962343531,"user_tz":-120,"elapsed":9936,"user":{"displayName":"Raphael Catanese","photoUrl":"","userId":"01157269626718155422"}},"outputId":"8e315b4c-9092-4b42-9179-8ce9c321b652"},"source":["train_x = train_x.view(12396,-1) # we could have written (12396,784) or (12396,28*28), but -1 is easier.\n","#It means, make this dimension as big as it should be to fit all the remaining data\n","#That's why we could have also writen (-1, 28*28)\n","train_y = train_y.view(-1,1)\n","train_x.shape, train_y.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([12396, 784]), torch.Size([12396, 1]))"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"markdown","metadata":{"id":"B0FBIY6bPmbQ"},"source":["We do the same transformation for the validation set"]},{"cell_type":"code","metadata":{"id":"BZx2TWoyPmij","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1602962343531,"user_tz":-120,"elapsed":9926,"user":{"displayName":"Raphael Catanese","photoUrl":"","userId":"01157269626718155422"}},"outputId":"5a7d7077-3651-43ea-9cd1-78a81a585069"},"source":["valid_x = valid_x.view(-1,28*28)\n","valid_y = valid_y.view(-1,1)\n","\n","valid_x.shape, valid_y.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([2038, 784]), torch.Size([2038, 1]))"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"markdown","metadata":{"id":"cN0Z1t1tPmyD"},"source":["Now we create our lists of tuples : "]},{"cell_type":"code","metadata":{"id":"1GvvjDN8Pm4D"},"source":["train_dataset = list(zip(train_x,train_y))\n","valid_dataset = list(zip(valid_x,valid_y))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AVvxGiljIelC"},"source":["And with that we can create Dataloaders : "]},{"cell_type":"code","metadata":{"id":"__xUl2sXIdpj"},"source":["train_dl = DataLoader(train_dataset, batch_size=100, Shuffle=True)\n","valid_dl = DataLoader(valid_dataset, batch_size=100, Shuffle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f00BL8aGLBYp"},"source":["Dataloaders creates batches of data form the tuples. The bigger the batches the more accurate the gradient is but also the longer it is to calculate. Without those batches we would train on all the data each time. \n","\n","Here is what those batches looks like : \n"]},{"cell_type":"code","metadata":{"id":"GxhZ8b7uLCmW","colab":{"base_uri":"https://localhost:8080/","height":444},"executionInfo":{"status":"ok","timestamp":1602962343532,"user_tz":-120,"elapsed":9906,"user":{"displayName":"Raphael Catanese","photoUrl":"","userId":"01157269626718155422"}},"outputId":"1b327701-bd39-4bc1-ac89-7cae165ccafe"},"source":["i = 0\n","for xb,yb in train_dl : \n","  print(xb)\n","  print(yb)\n","  i+=1\n","  if i>3 : \n","    break"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.]])\n","tensor([[1],\n","        [1],\n","        [1]])\n","tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.]])\n","tensor([[1],\n","        [1],\n","        [1]])\n","tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.]])\n","tensor([[1],\n","        [1],\n","        [1]])\n","tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.]])\n","tensor([[1],\n","        [1],\n","        [1]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"X8lEouFpm_MR"},"source":["# Initialization\n"]},{"cell_type":"markdown","metadata":{"id":"7ooLw1xXm_fu"},"source":["We now have our training and validation dataset. Next we have to initialize some weights and parameters for our model.\n","\n","We will create a function that will initialize the weights randomly with a normal distribution."]},{"cell_type":"code","metadata":{"id":"r7eZdNrJm_sp"},"source":["def param_init (size) :\n","  return (torch.randn(size)/100).requires_grad_() #I divided by 100 because I was saturating at the first epoch\n","  #It really changed everything ! The weight initialisation is really important and depends on the loss function we use (and on the architecture I suppose)\n","  #I also had to reduce the learning rate which was also causing the saturation"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M00ajTewm_-2"},"source":["\".requires_grad_()\" tells Pytorch that it will have to work out the gradient of those parameters\n","\n","For now our model has no hidden layers so we only need one weight for each pixel"]},{"cell_type":"code","metadata":{"id":"MnvHChyRnAGd","colab":{"base_uri":"https://localhost:8080/","height":106},"executionInfo":{"status":"ok","timestamp":1602969363644,"user_tz":-120,"elapsed":552,"user":{"displayName":"Raphael Catanese","photoUrl":"","userId":"01157269626718155422"}},"outputId":"2aa8f00a-0791-4a35-a68c-d6c31be16ee4"},"source":["weights = param_init((784,1))\n","weights[0:5]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[-0.0006],\n","        [-0.0032],\n","        [ 0.0052],\n","        [ 0.0035],\n","        [-0.0154]], grad_fn=<SliceBackward>)"]},"metadata":{"tags":[]},"execution_count":486}]},{"cell_type":"markdown","metadata":{"id":"6Y7uGC2NnAMm"},"source":["Now we initialize our bias : "]},{"cell_type":"code","metadata":{"id":"w9PS67F_nAS9"},"source":["bias = param_init(1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nW2GgVxlnAYd"},"source":["# A simple linear function"]},{"cell_type":"markdown","metadata":{"id":"IAVbDJk-rskw"},"source":["Now we will create our linear function : "]},{"cell_type":"code","metadata":{"id":"wpubSEX69PyS"},"source":["def linear(data) : \n","  return data@weights + bias # @ does matrix multiplication.\n","  #The data type of our training set is uint8 which is not supported by matrix multiplication so we must convert it to float "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ORt7wMbvaK8Q","colab":{"base_uri":"https://localhost:8080/","height":141},"executionInfo":{"status":"ok","timestamp":1602968903437,"user_tz":-120,"elapsed":437,"user":{"displayName":"Raphael Catanese","photoUrl":"","userId":"01157269626718155422"}},"outputId":"c5c9277f-b45c-4649-d06e-f76c1f059752"},"source":["linear(train_x)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[-0.1751],\n","        [-0.1815],\n","        [-0.0313],\n","        ...,\n","        [-0.0270],\n","        [-0.1397],\n","        [ 0.0701]], grad_fn=<AddBackward0>)"]},"metadata":{"tags":[]},"execution_count":427}]},{"cell_type":"code","metadata":{"id":"aCTgtTCg8-Oy"},"source":["i = 0\n","for xb,yb in train_dl : \n","  #print(linear(xb),yb)  #it makes more sense when the batch size is around 5\n","  i+=1\n","  if i>3 : \n","    break"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aupRlUQDJRZ2"},"source":["# Output function : sigmoid for a classification problem"]},{"cell_type":"code","metadata":{"id":"qhljh602JRhy"},"source":["def sigmoid(x): return 1/(1+torch.exp(-x))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QlbsQu_OJqPH","colab":{"base_uri":"https://localhost:8080/","height":281},"executionInfo":{"status":"ok","timestamp":1602968911267,"user_tz":-120,"elapsed":785,"user":{"displayName":"Raphael Catanese","photoUrl":"","userId":"01157269626718155422"}},"outputId":"3946f42f-a36c-495e-e6ed-4c3cc2c8eeaa"},"source":["plot_function(torch.sigmoid, title='Sigmoid', min=-4, max=4)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"fRwi76ltKGvi"},"source":["# Loss"]},{"cell_type":"code","metadata":{"id":"R_MEsqe9s5DY"},"source":["def mnist_loss1(predictions, targets):\n","    predictions = predictions.sigmoid()\n","    return torch.where(targets==1, 1-predictions, predictions).mean() # return 1-pred if target=1 (meaning it is a 3) and pred if not (it is a seven)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-Er1ieI2KHA2"},"source":["This loss function could do the job quite well. However we will use the Corss-Entropy loss function. Indeeed it is the most common loss function in classification problem and so if we want to know how to implement a neural network from scratch it makes sense to use it. "]},{"cell_type":"markdown","metadata":{"id":"0XPWP3ysz_pu"},"source":["It is good to know that Cross-Entropy is the most used loss function but is even better to understand why. The answer lies in the logarithme function."]},{"cell_type":"code","metadata":{"id":"eDt2CiXZ_yyP"},"source":["def log(x) : return -torch.log(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OZnpV9jD0k7G","colab":{"base_uri":"https://localhost:8080/","height":281},"executionInfo":{"status":"ok","timestamp":1602968914850,"user_tz":-120,"elapsed":775,"user":{"displayName":"Raphael Catanese","photoUrl":"","userId":"01157269626718155422"}},"outputId":"086b7899-08da-4755-f5a3-3d0bb1d7fd29"},"source":["plot_function(log, title='Logarithme', min=0, max=1,)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"6ssCkAb53X94"},"source":["We want to use the logarithme function because the sigmoid function saturate our neural network. It means that there will be little difference after the output function if the hidden layer were quite sure or almost absolutely sure of its prediction (the input of our output function will be a really high or really low number if the hidden layer are extremely sure of their prediction). \n","\n","Now we must ask ourself the aim of our classification. If we use accuracy as our metric, being quite sure or extremely sure of a good answer has no real importance and so we don't need our NN to update the most rapidly on this part. However we want our NN to update rapidly if it predict the wrong answer and the more wrong it is the faster we want to update it. \n","\n","To update our NN we use the gradient. So the stipper the slope the faster the update is. That is why the logatihme function is so usefull, we will use its first part to focus the update of our NN on the wrong answers. To benefits from these advantages of the logarithme function on our classification problems we use the Cross-Entropy loss function\n","\n","PS : we use -log(x) because we want the output to be positive. Moreover I personally find it more graphic as we could imagine the prediction of the output function as a ball dropped on our loss function and rolling down the cliff during the update of the weights. \n"]},{"cell_type":"code","metadata":{"id":"lWZQ9XP29_OA"},"source":["def cross_entropy(predictions,targets, epsilon = 0, penalty = 0.01) : #epsilon is here to prevent the divergence\n","  return -(targets*torch.log(predictions+epsilon) + (1-targets)*torch.log(1-predictions+epsilon)) #+ penalty*sum(weights) \n","  #the penalty makes the gradient way too long too calculate"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fKDxoeVcM7MQ"},"source":["def mnist_loss2(predictions,targets) : \n","  predictions = predictions.sigmoid() #.min(tensor([0.999999])).max(tensor([.000001])) #I have too much saturation. Beyond those limits I have 0 and 1 output \n","  list1 = [cross_entropy(x,y) for (x,y) in zip(predictions,targets)]\n","  #I imagine that there is a way to calculate the cross-entropy of each elements of a batch without using loops but I don't know it yet. \n","  #return tensor(sum(list1)/len(list1))\n","  return torch.stack(list1).mean()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r4NEH1mq0ODs"},"source":["We can use it because the output of our sigmoid function is included in ]0,1[\n","\"label\" and \"(1-label)\" has the same use as the target==1 condition in our loss function. ! HOWEVER if the initialization is wrongly handled and the ouput of the sigmoid is too close to 0 or 1 it will be rounded to 0 or 1 causing an error.\n","\n","-log(x) will make the prediction update toward 1 (as we can see with the direction of the slope) and -log(1-x) will make the prediction update toward 0. All in all the Cross-entropy makes the predictions update toward the right target. "]},{"cell_type":"markdown","metadata":{"id":"xPSjrX8Ydy4H"},"source":["We can test our loss function on the first 4 batch of our data loader. As you can see our loss without any training is quite horrible : "]},{"cell_type":"code","metadata":{"id":"l2xlDnXoQdat","colab":{"base_uri":"https://localhost:8080/","height":70},"executionInfo":{"status":"ok","timestamp":1602968920471,"user_tz":-120,"elapsed":536,"user":{"displayName":"Raphael Catanese","photoUrl":"","userId":"01157269626718155422"}},"outputId":"0e62e661-34ae-4a0d-fa39-230ba8fe78f1"},"source":["i = 0\n","#aa= 0\n","for xb,yb in train_dl : \n","  a = mnist_loss2(linear(xb),yb)\n","  #aa+=a\n","  print(a)\n","  i+=1\n","  if i>2 : \n","    break\n","#print(aa/i)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor(0.7425, grad_fn=<MeanBackward0>)\n","tensor(0.7406, grad_fn=<MeanBackward0>)\n","tensor(0.7378, grad_fn=<MeanBackward0>)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"40PoNNdjsvL4"},"source":["# Gradient"]},{"cell_type":"markdown","metadata":{"id":"tV0Gsq-Lszjt"},"source":["Now that we have our (huge) loss we can work on reducing it thanks to the gradient. To calculate our gradient we just have to call backard on our loss function. We will have the gradiant of the variables at the end of which we put \".requires_grad_()\" during our initialization : our weights and bias. "]},{"cell_type":"code","metadata":{"id":"lRG5PjdcnAeN"},"source":["def calc_grad(xb, yb, model):\n","    preds = model(xb)\n","    loss = mnist_loss2(preds, yb)\n","    loss.backward()\n","    return loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tzPsBQ2FGlaR","colab":{"base_uri":"https://localhost:8080/","height":70},"executionInfo":{"status":"ok","timestamp":1602968931830,"user_tz":-120,"elapsed":473,"user":{"displayName":"Raphael Catanese","photoUrl":"","userId":"01157269626718155422"}},"outputId":"56fa70db-cee3-4332-ebc0-dce3d258c374"},"source":["i = 0\n","for xb,yb in train_dl : \n","  a = calc_grad(xb, yb, linear)\n","  #print(a)\n","  print(weights.grad.mean(), bias.grad)\n","  i+=1\n","  #print(yb[0])\n","  if i>2 : \n","    break"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor(-0.0713) tensor([-0.5238])\n","tensor(-0.1445) tensor([-1.0467])\n","tensor(-0.2192) tensor([-1.5682])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ptKXpwQHnAxe"},"source":["# Training"]},{"cell_type":"markdown","metadata":{"id":"3ephe0EctBvu"},"source":["Now we will use the gradient to update our parameters."]},{"cell_type":"code","metadata":{"id":"if-5RVQqJVVp"},"source":["def train_epoch(dl, model, lr, params):\n","  tot_loss = []\n","  for xb,yb in dl:\n","    a =calc_grad(xb, yb, model)\n","    #print(weights.grad.mean(), bias.grad)\n","    #print(a)\n","    tot_loss.append(a)\n","    for p in params:\n","      p.data -= p.grad*lr #the attribute data tells Pytorch to not take this step into consideration while calculating the gradient\n","      p.grad.zero_() #each time we do loss.backward we add the gradient to the existing gradient of the variable. Thaht's why we must put the gradient to 0\n","  return torch.stack(tot_loss).mean().item() #item() remove the tensor sign"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uGh2kFN1pxvw"},"source":["lr = 0.001\n","params = weights, bias #it will be easier when we will change the number of layers"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YNnR6ct4kHUh","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1602969383357,"user_tz":-120,"elapsed":1972,"user":{"displayName":"Raphael Catanese","photoUrl":"","userId":"01157269626718155422"}},"outputId":"f7e83f2f-f6fc-45a9-c633-5e08db7623a8"},"source":["train_epoch(train_dl, linear, lr, params)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.5686315298080444"]},"metadata":{"tags":[]},"execution_count":489}]},{"cell_type":"code","metadata":{"id":"ffcHbPBhnA3-","colab":{"base_uri":"https://localhost:8080/","height":372},"executionInfo":{"status":"ok","timestamp":1602969418454,"user_tz":-120,"elapsed":29840,"user":{"displayName":"Raphael Catanese","photoUrl":"","userId":"01157269626718155422"}},"outputId":"da855204-8030-447e-b28c-709bf1b1c33d"},"source":["for i in range(20):\n","    print(train_epoch(train_dl, linear, lr, params))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.4658467769622803\n","0.3928288519382477\n","0.3407185673713684\n","0.3027758300304413\n","0.27425774931907654\n","0.25213998556137085\n","0.23450878262519836\n","0.22012470662593842\n","0.2081591784954071\n","0.19804082810878754\n","0.18936432898044586\n","0.1818346381187439\n","0.17523208260536194\n","0.16939006745815277\n","0.16417963802814484\n","0.15949970483779907\n","0.1552698165178299\n","0.15142527222633362\n","0.1479130983352661\n","0.1446898728609085\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4_2kFpBZ4RG3"},"source":["We did it ! Our model has learned. However the loss is not what we will use at the end to evaluate our model. To do so we will use the accuracy as the metric of our model. "]},{"cell_type":"code","metadata":{"id":"GkzYiXnrInq5","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1602969067147,"user_tz":-120,"elapsed":610,"user":{"displayName":"Raphael Catanese","photoUrl":"","userId":"01157269626718155422"}},"outputId":"fdce1e3a-2608-42db-de27-faf20939b3a6"},"source":["sum(abs(weights)) #Putting some constraints on the weight (through the loss function or directly on them) could maybe help but it takes too much time to calculate then"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([19.2222], grad_fn=<AddBackward0>)"]},"metadata":{"tags":[]},"execution_count":456}]},{"cell_type":"markdown","metadata":{"id":"tcr5KwIM8NuD"},"source":["# Metric"]},{"cell_type":"markdown","metadata":{"id":"n0WPGNLD-8fP"},"source":["Now we need a metric to see how good our prediction is. We will use accuracy as the metric of this model : "]},{"cell_type":"code","metadata":{"id":"gdYnWmKC8N5a"},"source":["def batch_accuracy(xb, yb):\n","    preds = xb.sigmoid()\n","    correct = (preds>0.5) == yb\n","    return correct.float().mean()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SP6u4dQ2i77J"},"source":["def epoch_accuracy(model, valid_dl = valid_dl):\n","    accs = [batch_accuracy(model(xb), yb) for xb,yb in valid_dl] #what matters is the accuracy on the validation dataset\n","    return torch.stack(accs).mean().item()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6boeKejmtqya"},"source":["#Reinitilization\n","weights = param_init((784,1))\n","bias = param_init(1)\n","params = weights, bias\n","lr = 0.01"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cuGpm78H6xTs","colab":{"base_uri":"https://localhost:8080/","height":372},"executionInfo":{"status":"ok","timestamp":1602969589650,"user_tz":-120,"elapsed":29939,"user":{"displayName":"Raphael Catanese","photoUrl":"","userId":"01157269626718155422"}},"outputId":"39a5b7dc-7ef7-4269-8a6e-0b1f12bf56a2"},"source":["for i in range(20):\n","    print(round(train_epoch(train_dl, linear, lr, params),8), round(epoch_accuracy(linear),5))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.23092026 0.54333\n","0.18861566 0.89113\n","0.14180546 0.93779\n","0.1212466 0.95827\n","0.10929321 0.96208\n","0.10127825 0.96351\n","0.09542577 0.96494\n","0.09090401 0.96637\n","0.08726787 0.96684\n","0.08425561 0.96779\n","0.08170252 0.96875\n","0.07949898 0.96922\n","0.07756916 0.97018\n","0.07585841 0.97065\n","0.07432652 0.97113\n","0.0729429 0.9716\n","0.07168397 0.9713\n","0.07053116 0.9713\n","0.06946959 0.97178\n","0.06848718 0.97178\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0_JizyQk_F1-"},"source":["# Activation function. Adding non-linearity : the universal approximation problem"]},{"cell_type":"markdown","metadata":{"id":"8nMlB4p8_GVD"},"source":["Our accuracy is still not that great and we still miss one of the basics of NN : non-linearity. So let's add a ReLU function to our model"]},{"cell_type":"code","metadata":{"id":"ouQOHKlJ_GeD"},"source":["def model1(data) : \n","  preds = data@w1 + b1 #it is our linear function but we will give it more than one output\n","  preds = preds.max(tensor(.0)) #Our ReLU function (we put .0 because we need a float)\n","  preds = preds@w2 + b2\n","  return preds"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"htVP-usGC5J2"},"source":["w1 = param_init((28*28,28))\n","b1 = param_init(28)\n","w2 = param_init((28,1))\n","b2 = param_init(1)\n","params = w2, b2, w1, b1 #it is EXTREMELY important to put them in a descendant order because of how  the calculus of the gradient is handled\n","#(form the last layers to the first ones) "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZxVFK80qDssF"},"source":["Let's test if there are improvement : "]},{"cell_type":"code","metadata":{"id":"pFt_iwt_Ds0z","colab":{"base_uri":"https://localhost:8080/","height":372},"executionInfo":{"status":"ok","timestamp":1602969637742,"user_tz":-120,"elapsed":30780,"user":{"displayName":"Raphael Catanese","photoUrl":"","userId":"01157269626718155422"}},"outputId":"0be91ebe-eb14-40ea-dc2e-fbba2b5df865"},"source":["for i in range(20):\n","    print(round(train_epoch(train_dl, model1, lr, params),8), round(epoch_accuracy(model1),5))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.67953861 0.79095\n","0.57774252 0.86762\n","0.31692496 0.83571\n","0.17113507 0.91398\n","0.11934368 0.94113\n","0.09702036 0.95875\n","0.08540333 0.96351\n","0.07823346 0.96398\n","0.07327435 0.96637\n","0.06955853 0.96732\n","0.06661744 0.96779\n","0.06419647 0.9697\n","0.06213756 0.97065\n","0.06035793 0.97065\n","0.05879267 0.97113\n","0.05740786 0.97256\n","0.0561594 0.97398\n","0.05502799 0.97446\n","0.05399564 0.97446\n","0.05304195 0.97398\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"iWrPFnFXJVK9"},"source":["This model is indeed better than the other !"]},{"cell_type":"markdown","metadata":{"id":"UHcCELXpnBYZ"},"source":["# The End"]},{"cell_type":"markdown","metadata":{"id":"SkCvagbrKU4i"},"source":["That's it we created a simple neural netwerk in python ! "]},{"cell_type":"markdown","metadata":{"id":"8q5pvLqOKfQr"},"source":["THe next steps I could try (and I surely will in the future) : \n","-Record the weights at each epoch\n","-Stop when the metrics is worsening too much and return the parameters which gave the best accuracy on the validation set\n","-Put more illustrations\n","-Build a mini-CNN and a mini-RNN\n"]}]}